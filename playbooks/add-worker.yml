---

- hosts: all
  gather_facts: true
  become: true
  #any_errors_fatal: true

  pre_tasks:
  #- name: Create dirs in "{{lookup('env', 'HOME')}}/" on localhost
  #  file:
  #    path: "{{item}}"
  #    state: directory
  #  run_once: true
  #  become: false
  #  delegate_to: localhost
  #  with_items:
  #    - "{{lookup('env', 'HOME')}}/.local/bin/"
  #    - "{{lookup('env', 'HOME')}}/.kube"

  #- name: Check kubectl on localhost
  #  stat: 
  #    path: "{{lookup('env', 'HOME')}}/.local/bin/kubectl"
  #  register: _kctl_local
  #  run_once: true
  #  delegate_to: localhost
  #  become: false

  #- name: Get kubectl version from master
  #  shell: kubectl version|tr ',' '\n'|egrep GitVersion|head -1|cut -d':' -f2|xargs
  #  register: _kubectl_v
  #  run_once: true
  #  when: not _kctl_local.stat.exists and node_type == "master"
      
  #- name: Download kubectl binary on localhost
  #  get_url:
  #    url: "https://dl.k8s.io/release/{{_kubectl_v.stdout}}/bin/linux/amd64/kubectl"
  #    dest: "{{lookup('env', 'HOME')}}/.local/bin/"
  #    mode: 0755
  #  delegate_to: localhost
  #  become: false
  #  run_once: true
  #  when: not _kctl_local.stat.exists

  #- name: Fetch kube config from master to localhost
  #  fetch:
  #    src: "/home/{{ansible_user}}/.kube/config"
  #    dest: "{{lookup('env', 'HOME')}}/.kube/"
  #    flat: yes
  #  when: node_type == "master"
  #  become: false
  #  run_once: true
     
  - name: Remove /tmp/node-status.yml
    file:
      path: /tmp/node-status.yml
      state: absent
    become: false
    when: node_type == "master"

  - name: Append cluster nodes status in /tmp/node-status.yml
    shell: |
      [ -f /tmp/node-status.yml ] && echo "/tmp/node-status.yml already exists" && exit 1
      echo "nodes_list:" > /tmp/node-status.yml
      for n in $(kubectl get nodes|awk '{print $1}'|egrep -v NAME);do
        echo "  - $n" >> /tmp/node-status.yml
      done
      echo >> /tmp/node-status.yml
      echo "nodes_status:" >> /tmp/node-status.yml
      
      for n in $(kubectl get nodes|awk '{print $1}'|egrep -v NAME);do
        if kubectl get nodes --field-selector metadata.name=$n|egrep Ready;then
          echo "  - host: {{inventory_hostname}}" >> /tmp/node-status.yml
          echo "    ready: true" >> /tmp/node-status.yml
        else
          echo "  - host: {{inventory_hostname}}" >> /tmp/node-status.yml
          echo "    ready: false" >> /tmp/node-status.yml
        fi
      done
    become: false
    when: node_type == "master"
    args:
      chdir: "/home/{{ansible_user}}"

  - name: Fetch /tmp/node-status.yml from master
    fetch:
      src: /tmp/node-status.yml
      dest: /tmp/
      flat: yes
    become: false
    when: node_type == "master"

  - name: Include vars from /tmp/node-status.yml
    include_vars:
      file: /tmp/node-status.yml

  - debug:
      var: nodes_list 
    run_once: true

  - debug:
      var: nodes_status  
    run_once: true

  - name: Set fact kube_node_status for existing worker nodes
    set_fact:
      kube_node_status: "{{item.ready}}"
    with_items: "{{nodes_status}}"
    when: item.host == inventory_hostname

  - name: Set fact kube_node_status for new worker nodes
    set_fact:
      kube_node_status: "false"
    when: 
      - node_type == "worker"
      - inventory_hostname not in nodes_list
  
  - debug:
      var: kube_node_status

  - name: Remove existing Ready worker node from play_hosts
    meta: end_host
    when: 
      - node_type == "worker"
      - kube_node_status|bool
    
  - name: Abort playbook if no worker node left to add in cluster
    meta: end_host
    when: ansible_play_hosts is not search('worker')

  - name: Added and append new_nodes var in /tmp/node-status.yml
    shell: |
      echo >> /tmp/node-status.yml
      echo "new_nodes:" >> /tmp/node-status.yml
      echo "  - {{inventory_hostname}}" >> /tmp/node-status.yml
    when: node_type == "worker"
    delegate_to: localhost
    become: false
    
  - name: Include vars from /tmp/node-status.yml
    include_vars:
      file: /tmp/node-status.yml

  - debug:
      var: new_nodes
    run_once: true

  tasks:
  - block:
    - name: Create /etc/apt/keyrings
      file:
        path: /etc/apt/keyrings
        state: directory
        mode: 0755
        owner: root
        group: root
    
    - name: Get stat of /etc/apt/keyrings/docker.gpg
      stat:
        path: /etc/apt/keyrings/docker.gpg
      register: _docker_gpg

    - name: Install Docker GPG key
      shell: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
      when: not _docker_gpg.stat.exists

    - name: Setup docker reposiroty
      shell: |
        echo  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
        $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

    - name: Install docker packages
      apt:
        name:
          - docker.io
          - containerd 
        autoremove: yes
        update_cache: yes
        state: latest
    when: node_type == "worker"

  - name: swapoff
    command: swapoff -a
    when: node_type == "worker"

  - name: Fetch files from master
    fetch:
      src: "{{item}}"
      dest: /tmp/
      flat: yes
    with_items:
      - /etc/docker/daemon.json
    when: node_type == "master"
    run_once: true

  - name: Copy docker daemon json on worker nodes
    copy:
      src: /tmp/daemon.json
      dest: /etc/docker/
      mode: 0644
      owner: root
      group: root
    when: node_type == "worker"

  - name: Get kubeadm version from master
    shell: apt list -a kubeadm|awk '/now/{print $2}'
    register: _kadm_version
    when: node_type == "master"
    run_once: true

  - debug:
      var: _kadm_version.stdout
    run_once: true

  - name: Install kube packages on worker nodes
    apt: 
      name:
        - "{{item}}={{_kadm_version.stdout}}"
      state: present
      update_cache: yes
      allow_downgrade: yes
    with_items:
      - kubeadm
      - kubelet
    when: node_type == "worker"
    
  - name: Cleanup existing files
    file:
      path: /etc/kubernetes
      state: absent
    when: node_type == "worker"

  - name: Stop kubelet
    systemd:
      name: kubelet
      state: stopped
    when: node_type == "worker"
    
  - name: Restart docker
    systemd:
      name: docker
      state: restarted
    when: node_type == "worker"

  - name: Generate token
    shell: 'kubeadm token create --print-join-command'
    register: _kadm_token
    when: node_type == "master"
    run_once: true

  - debug:
      var: _kadm_token.stdout
    when: node_type == "master"
    run_once: true

  - name: Run kubeadm join on worker node
    shell: '{{_kadm_token.stdout}}'
    when: node_type == "worker"

  - name: Check Ready status of new added node
    shell: kubectl get nodes --field-selector metadata.name={{item}}|egrep -w Ready
    register: _node_ready
    retries: 5
    delay: 10
    until: _node_ready.rc == 0
    when: node_type == "master"
    become: false
    with_items: "{{new_nodes}}"

  - name: Set Worker nodes label to worker
    shell: kubectl label node {{item}} node-role.kubernetes.io/worker=worker
    when: node_type == "master"
    become: false
    with_items: "{{new_nodes}}"
    
  - name: Get nodes list
    shell: kubectl get nodes
    when: node_type == "master"
    become: false
    register: _node_list

  - debug:
      msg:
        - "Kubernetes cluster nodes list:"
        - "{{_node_list.stdout_lines}}"
    run_once: true
